## 深入分词

[官网Text analysis » Text analysis overview](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-overview.html)

分词: 文本按照一定规则进行拆解, 分成多个独立词项. 搜索时基于分词后的字符去检索

es中分词只针对text类型. keyword不分词或者说分成一个词

> text分词后会根据词项加入[倒排索引](./常用查询函数/Match文本分词查询.md#倒排索引)

修改字段分词器需要重建索引!



## 测试API

`GET <index>/_analyze`



### 测试简单分词器

```json
// 分词器
GET _analyze
{
  "text": [
    "HELLO WORD, i am 陈先生007. 2022-02-02"
  ],
  "analyzer": "fingerprint"
}

// 自定义分词器
GET _analyze
{
  "text": [
    "one t-w-o"
  ],
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "- => "
      ]
    }
  ],
  "tokenizer": "standard",
  "filter": [
    "uppercase"
  ]
}
```



### 测试复杂自定义分词器

有一些负责的分词器不能直接在在`_analyze`里配置. 需要创建索引, 然后指定索引里的分词器才能测试

```json
PUT text_001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "char_filter": ["my_char_filter"],
          "filter": [
            "my_stop",
            "synonym"
          ]
        }
      },
      "char_filter": {
        "my_char_filter": {
        "type": "mapping",
        "mappings": [
          "- => "
          ]
        }
      }, 
      "filter": {
        "my_stop": {
          "type": "stop",
          "stopwords": [
            "one"
          ]
        },
        "synonym": {
          "type": "synonym",
          "lenient": true,
          "synonyms": [
            "foo, bar => baz",
            "two, 2 => two, 2"
          ]
        }
      }
    }
  }
}

// 测试
GET text_001/_analyze
{
  "analyzer": "my_analyzer", 
  "text": [
    "one t-w-o"
  ]
}
```





## 常用内置分词器

测试分词语句`"HELLO WORD, i am 陈先生007. 2022-02-02"`

* **standard**

  标准分词器, 按照Unicode文本分词. 会小写字母并支持停用词

  > ["hello", "word", "i", "am", "陈", "先", "生", "007", "2022", "02", "02"]

* **simple**

  简单分词器. 遇到非英文字符时分词. 会小写分词

  > ["hello", "word", "i", "am", "陈先生"]

* **keyword**

  关键字分词器. 将所有文本分为一个词

  > ["HELLO WORD, i am 陈先生007. 2022-02-02"]

* **pattern**

  按照正则分词, 默认正则`\W+`(表示匹配数字、字母、下划线和加号本身字符)

  使用时需要注意. 如果正则复杂将会允许很慢甚至抛出异常!

* **stop**

  停用词分词器. simple+英语停用词(a, the之类的会删除)

  > ["hello", "word", "i", "am", "陈先生"]

* **whitespace** 

  空白分词器. 遇到空白字符时会将文本划分成词项

  > ["HELLO ", "WORD", "i", "am", "陈先生007", "2022-02-02"]

* **fingerprint**

  指纹分词器. 小写文本, 删除拓展字符, 并对文本进行排序去重. 然后分成单个词

  > ["007 02 2022 am hello i word 先 生 陈"]
  >
  > 不知道用在什么场景...

* **language**

  内置的多种语言分词器, 需要自定义参数



## 分词器组成

* **Character filters** 0-n个

  字符过滤器

* **Tokenizer** 只有1个

  词项分析分词

* **Token filter** 0-n个

  词项过滤

> 入库和查询时都会经过这3个组件



### Character filters

[官网Text analysis » Character filters reference](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-charfilters.html)

字符过滤器, 能过滤特定字符, 内置3个过滤器. 关键字: **char_filter**

* **HTML strip**

  过滤html标签, 并替换转义字符(`&apos;` => `'` )

  `escaped_tags`: (array) 可以选择跳过指定的html标签

* **Mapping**

  能转换特定字符

  `mappings`: 转换的字符数组. 格式为`{key} => {value}`

  `mappings_path`: 映射文件的绝对路径

  ```json
  GET /_analyze
  {
    "char_filter": [
      {
        "type": "mapping",
        "mappings": [
          "a => 0",
          "- =>  "
        ]
      }
    ],
    "text": "aa---aaa"
  }
  ```

* **Pattern replace**

  正则过滤器

  `pattern`: 正则

  `replacement`: 替换的字符



### Tokenizer

[官网Text analysis » Tokenizer reference](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-tokenizers.html)

词项分析器. 分词器的主体, 用于拆分文本. 关键字: **tokenizer**. 内置有以下

测试分词语句`"HELLO WORD, i am 陈先生007. 2022-02-02"`

* **ngram**

  滑动窗口分词. 

  > "one" => ["o", "on", "n", "ne", "e"]

* **edge_ngram**

  类似ngram. 但会锚定单词的开头.

  对于启发式搜索很有用! 

  > "one" => ["o", "on", "one"]

* **standard**

  标准分词. 基于Unicode文本分段算法

  > ["HELLO", "WORD", "i", "am", "陈", "先", "生", "007", "2022", "02", "02"]

* **letter**

  字母分词器. 遇到非字母时拆分

  > ["HELLO", "WORD", "i", "am", "陈先生"]

* **lowercase**

  和letter类似, 不过会把大写字母转换为小写

  > ["hello", "word", "i", "am", "陈先生"]

* **whitespace**

  空白字符分词器

  > ["HELLO", "WORD", "i", "am", "陈先生007.", "2022-02-02"]

* **uax_url_email**

  在standard的基础上增加url和邮件地址分词

* **keyword**

  不分词, 或者说分为一个词

* **pattern**

  正则分词

* **simple_pattern**

  正则分词, 需要固定结果集个数. 匹配结果为词项

* **char_group**

  固定字符分词. 接受一组字符, 按字符分词.

* **simple_pattern_split**

  和simple_pattern类似, 但匹配结果用于分隔符

* **path_hierarchy**

  文件系统路径分词

  > "/foo/bar/baz" =>  ["/foo", "/foo/bar", "/foo/bar/baz" ]



### Token filter

[官网Text analysis » Token filter reference](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-tokenfilters.html)

词项过滤. 对词项进行修改/过滤/添加同义词等操作. 关键字: **filter**

官网有很多内置操作. 不细写...

```json
GET _analyze
{
  "text": [
    "one t-w-o"
  ],
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "- => "
      ]
    }
  ],
  "tokenizer": "standard",
  "filter": [
    "uppercase" //转大写
  ]
}
```



## 自定义分词器

1. 在settings中自定义分词器

2. 在text中指定. 需要注意的是有2个分词时机. 

   `analyzer`: 入库时的分词器

   `search_analyzer`: 搜索时的分词器

​		

**示例**

```json
PUT common-test-001
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    // 自定义分词器
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "tokenizer": "ik_max_word",
          "char_filter": [
            "html_strip"
          ],
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "nameText": {
        "type": "text",
        // 指定分词器
        "analyzer": "my_analyzer",
        "search_analyzer": "my_analyzer"
      },
      "nameKeyword": {
        "type": "keyword"
      },
      "intRange": {
        "type": "integer_range"
      }
    }
  }
}
```





## IK分词器安装

[IK GitHub](https://github.com/medcl/elasticsearch-analysis-ik)

在es目录下运行即可, 需要修改版本号

```shell
./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip
```



IK自带2个分词器

* **ik_max_word**: 会将文本做最细粒度的拆分, 比如会将"中华人民共和国国歌"拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,国,国歌”, **会穷尽各种可能的组合**, 适合 Term Query
* **ik_smart**: 会做最粗粒度的拆分, 比如会将"中华人民共和国国歌"拆分为"中华人民共和国, 国歌"，适合 Phrase 查询。

​		

## 现代搜索引擎

现代搜索引擎都是: 分词 + 统计 + 机器学习(后续可能就是人工智能)

做到千人千面的搜索

