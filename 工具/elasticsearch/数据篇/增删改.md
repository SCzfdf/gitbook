# 增删改

ES中存在2个ID, 一个是ESId, 一个是LuceneId

在创建时指定和元数据中展示的是ESId. LuceneId为一个自增的int型Id, 不可以指定. 

> 因此单个分片上最能容纳2^32-1条数据



## 数据更新路由机制

索引是分片设计, 路由默认基于数据id计算出需要写入的分片. 

优先写入到主片, 再同步到副本分片

> 大批量数据导入时可以临时禁用副本分片, 减少同步时间. 等录入完成后再慢慢同步

![ES数据写入路由.drawio](%E5%A2%9E%E5%88%A0%E6%94%B9.assets/ES%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E8%B7%AF%E7%94%B1.drawio.svg)

> 使用的算法是Murmur3算法





## 数据更新逻辑

和其他数据产品差不多.

先写buffer, buffer满了就提交到文件系统. 再依靠文件系统刷新到硬盘

> 因此settings.refresh_interval的配置不是精确的. 

![ES数据写入逻辑.drawio](%E5%A2%9E%E5%88%A0%E6%94%B9.assets/ES%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E9%80%BB%E8%BE%91.drawio.svg)

> `POST <target>/<operation>/?refresh=true `
>
> `POST <target>/_flush` 
>
> 执行完2个命令之后就可以看做是入到磁盘了



## 数据增删改流程

ES更新的本质就是删除+增加

![image-20220504173132234](%E5%A2%9E%E5%88%A0%E6%94%B9.assets/image-20220504173132234.png)



## 数据写入



### 数据单条写入(Index API)

[官网REST APIs » Document APIs » Index API](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-index_.html#docs-index_)

单条数据写入有4种方式

1. `PUT /<target>/_doc/<_id>`
2. `POST /<target>/_doc/`
3. `PUT /<target>/_create/<_id>`
4. `POST /<target>/_create/<_id>`

在不使用`_create`时PUT可以用于插入和更新(id存在则更新), POST只能用于插入(id由es决定)

> 如果新增数据中有新增字段, 则会刷新索引mapping结构. 尽量避免



#### Index API常用参数

* **op_type**: (enum)

  如果指定了<_id>则默认`index`, 否则默认`create`

  `create`: 不存在就创建, 如果指定<_id>存在则失败

  `index`: 不存在就创建, 存在则更新

* **refresh**: (enum)

  本次操作是否马上刷新. 默认`false`

  `true`: 马上刷新分片, 使本次更新能被搜索的到(手动提交buffer)

  `false`: 不做操作. 

  `wait_for`: 阻塞到数据刷新

* **routing**: (string)

  使用特定的字符进行路由, 以便能够将本次操作路由到特定分片

  **routing不能随便使用**

  ```json
  // 只修改routing值, 发送多次的话会创建多个id为1的文档!
  POST common-test-001/_doc/1?routing=3
  {
  	"name": "xx"
  }
  ```

* **wait_for_active_shards**: (all | integer)

  默认值1(主分片成功即可)

  在操作必须等待特定的分片数量写入成功才返回

* **timeout** ([time](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/api-conventions.html#time-units))

  只能确保在失败前最少等待时间(实际上可能等待更久)

* **pipeline** (ingest pipeline)

  预处理管道id
  
* **version**(integer)和**version_type**(enum)

  [ElasticSearch 并发的处理方式](https://blog.51cto.com/u_15080000/2593390)

  默认`internal`: es会自己维护版本号, 不用通过`version`显式传

  `external`和`external_gt`的效果是一样的: `version`的版本号应该**大于**上次的`version`

  `external_gte`: `version`的版本号应该**大于等于**上次的`version`

  `version`只接受整数类型.

* **if_seq_no**(integer)和**if_primary_term**(integer)

  `seq_no`: 索引更新序列号, 操作一次分片中的数据就+1

  `primary_term`: 主分片任期编号, 选举一次就+1

  通过对比这2个参数可以防止在并发时旧数据覆盖新数据
  
  > `seq_no`和`primary_term`可以通过查询获取
  >
  > 不知道用在什么场景, 感觉version比较好用点



## 数据删除

ES采用的是标记删除, 收到删除请求后会记录到一个索引中, 等下次磁盘刷新时才真正删除

> 感觉这些大数据量的程序都是一个套路, kafka也是. 都是有一个buffer, 满了才执行操作



### 数据删除(Delete API)

[官网REST APIs » Document APIs » Delete API](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-delete.html)

* `DELETE /<index>/_doc/<_id>`

  根据id删除 

  

#### Delete API常用参数

Delete API的参数与[Index API](#Index API常用参数)基本一致

​		

### 数据批量删除(delete_by_query)

[官网REST APIs » Document APIs » Delete by query API](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-delete-by-query.html)

* `POST /<target>/_delete_by_query`

  基于搜索条件删除, 由ES自己调度完成, 实际上走的是scroll查询

  > 可以通过`GET _cat/tasks?v` 查看正在进行的任务

  ```json
  POST common-test-001/_delete_by_query
  {
    "query": {
      // 如果要删除索引的所有数据应该重建索引
      "match_all": {}
    }
  }
  ```



#### delete_by_query常用参数

* **requests_per_second**: (integer)

  默认-1(不控制). 控制每秒查询(删除)的数量. 防止集群io过大影响其他业务

* **slices**: (integer)

  默认1(没有被切成子任务). 控制这个任务分为几个子任务

* **scroll_size**: (integer)

  默认1000. 滚动查询的大小

* **scroll**: (integer)

  为滚动保留上下文的周期

* **conflicts**: (enum)

  默认`abort`

  `abort`: 遇到版本冲突时终止

  `proceed`: 遇到版本冲突时继续





## 数据更新

ES在多并发下的更新操作**无事务隔离**. 因此高并发下会产生严重的数据错位



### 数据单条更新(Update API)

[官网REST APIs » Document APIs » Update API](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-update.html)

如果新增数据中有新增字段, 则会刷新索引mapping结构. 尽量避免

* `POST /<index>/_update/<_id>` (**推荐使用**, 避免并发丢数据)

  局部更新(只会更新doc里面的字段, doc没写的就保留. 虽然是局部更新, 但依旧是先删后加)

  **索引`_source`必须为true**

  ```json
  POST common-test/_update/1
  {
    "doc": {
      "status": "4",
      "name": "李四"
    },
    // 默认false. 为true时更新的id不存在就改为create操作
    "doc_as_upsert": true,
    
    // "script": {
    //  "source": """
    //  ctx._source.status=ctx._source.status+params.p1
    //  """,
    //  "params": {
    //    "p1": "11"
    // }, 
    // 默认false同doc_as_upsert. 注意的是id不存在, 不是字段不存在
    // "scripted_upsert": true,
    // "upsert": {
    //  "status": "asd"
    // }
    }
  }
  ```

* `PUT <target>/_doc/<_id> (Index API)`

  覆盖式的更新(执行后请求体是怎样的文档就是怎样的)

  ```json
  POST common-test-001/_doc/1
  {
    "status": "3",
    "name": "李四"
  }
  ```



#### 删除文档中的某个字段

使用脚本

```json
POST common-test/_update/1
{
  "script": {
    "source": """
    // ctx._source其实就是一个map
    ctx._source.remove('status')
    """
  }
}
```

也可以用覆盖更新, 程序中一般置为null就足够了



#### 全量更新

假如有一个数据量很大的索引需要全量更新. 此时就不推荐使用update. 而应该使用reindex

因为update需要查询一次, 如果索引太大的话更新时间太长反而是小问题, 更危险的是提交线程一直在阻塞. 并且版本频繁冲突



#### Update API常用参数

Update API的参数与[Index API](#Index API常用参数)基本一致

* **lang**: (enum)

  指定更新的脚本语言. 默认`painless`

* **retry_on_conflict**: (integer)

  默认0. 发生冲突时的重试次数(一般是并发下的版本冲突)



### 数据批量更新(update_by_query)

[官网REST APIs » Document APIs » Update by query API](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html)

使用update_by_query最好让脚本兼容性好一些(多判断属性是否存在之类的). 因为没有es没有事务, 如果脚本不支持幂等就会很麻烦

​		

* `POST /<target>/update_by_query`

  基于搜索条件删除, 由ES自己调度完成, 实际上走的是scroll查询

  ```json
  POST common-test-001/_update_by_query
  {
    "script": {
      "source": "ctx._source.count++",
      "lang": "painless"
    },
    "query": {
      "term": {
        "user.id": "kimchy"
      }
    }
  }
  ```



#### update_by_query常用参数

update_by_query的参数与[delete_by_query](#delete_by_query常用参数)基本一致











